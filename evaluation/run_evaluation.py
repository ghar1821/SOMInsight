#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Mar  5 12:40:16 2021

Evaluate features generated by edgeR and limma pairwise.

@author: givanna
"""

import pandas as pd
import json
import os

from classifiers_functions import run_knn, run_nb, run_dt, run_svm, run_rf, run_logistic_regression, normalise_to_0_1

# read patient details
pat_dets = pd.read_csv("M:/givanna/cmv/patient_groups.csv")
pat_dets = pat_dets.set_index("PatientName")
# fold_id = 'fold_1'

save_dir = "M:/givanna/cmv/diffcyt_10x10_20_completeTP_marker_from_LS/classification_res"

if not os.path.exists(save_dir):
    os.makedirs(save_dir)

all_res = pd.DataFrame(columns=['Classifier', 'Acc_train', 'Acc_test', 'Fold', 'Best_param'])
conf_mat = pd.DataFrame(columns=['Predicted', 'Reference', 'Classifier', 'Fold'])

for fold_idx in range(1, 27):
    fold_id = 'fold_' + str(fold_idx)

    print("Processing " + fold_id)
    
    # TODO adjust me accordingly
    # dat_dir = "M:/givanna/cmv/tracksom_10x10_20_completeTP_marker_from_LS/lmms_features_NAmedian_"+ str(feat_num) + "features/" + fold_id + "/"
    dat_dir = "M:/givanna/cmv/diffcyt_10x10_20_completeTP_marker_from_LS/" + fold_id + "/"

    # TODO adjust me accordingly
    # train_dat = pd.read_csv(dat_dir + "/train_data_imp_feats_prop_linreg.csv")
    # test_dat = pd.read_csv(dat_dir + "/test_data_imp_feats_prop_linreg.csv")
    train_dat = pd.read_csv(dat_dir + "/train_data_6imp_feats_diffcyt.csv")
    test_dat = pd.read_csv(dat_dir + "/test_data_6imp_feats_diffcyt.csv")
    
    train_dat = train_dat.join(pat_dets, on='PatientName')
    test_dat = test_dat.join(pat_dets, on='PatientName')
    
    print("isolating features")
    # isolate just features
    train_dat_feats = train_dat.drop(['PatientName', 'Group', 'group_reactive'], axis=1)
    test_dat_feats = test_dat.drop(['PatientName', 'Group', 'group_reactive'], axis=1)
    
    
    # test data may contain missing values, if the cluster does not contain
    # data for the patient. So just treat as 0 as NA is not accepted.
    missing_feats = list(set(train_dat_feats.columns) - set(test_dat_feats.columns))
    test_dat_feats[missing_feats] = 0
    
    # reorder test data column to match train data
    test_dat_feats = test_dat_feats[train_dat_feats.columns]
    
    # convert nan features to 0. ONLY USE IF NECESSARY
    train_dat_feats = train_dat_feats.fillna(0)
    test_dat_feats = test_dat_feats.fillna(0)
    
    # isolate the patient groups
    train_dat_grps = train_dat[['group_reactive']].to_numpy().ravel()
    test_dat_grps = test_dat[['group_reactive']].to_numpy().ravel()
    
    # Not needed if range is already constrained
    train_dat_feats_norm, test_dat_feats_norm = normalise_to_0_1(
        train_dat = train_dat_feats,
        test_dat = test_dat_feats)
    
    # run classifiers
    knn_res = run_knn(train_dat=train_dat_feats_norm, 
                          train_label=train_dat_grps, 
                          test_dat=test_dat_feats_norm, 
                          test_label=test_dat_grps)
        
    nb_res = run_nb(train_dat=train_dat_feats_norm, 
                      train_label=train_dat_grps, 
                      test_dat=test_dat_feats_norm, 
                      test_label=test_dat_grps)
    
    dt_res = run_dt(train_dat=train_dat_feats_norm, 
                      train_label=train_dat_grps, 
                      test_dat=test_dat_feats_norm, 
                      test_label=test_dat_grps)
    
    # this because SVM may not converge, and the warnings are irritating as 
    # it advices pre-processing, but the data has been pre-processed! 
    # bloody sklearn.
    svm_res = run_svm(train_dat=train_dat_feats_norm, 
                      train_label=train_dat_grps, 
                      test_dat=test_dat_feats_norm, 
                      test_label=test_dat_grps)
    
    rf_res = run_rf(train_dat=train_dat_feats_norm, 
                      train_label=train_dat_grps, 
                      test_dat=test_dat_feats_norm, 
                      test_label=test_dat_grps)
    
    lr_res = run_logistic_regression(train_dat=train_dat_feats_norm, 
                      train_label=train_dat_grps, 
                      test_dat=test_dat_feats_norm, 
                      test_label=test_dat_grps)
    
    all_class_res = [knn_res, nb_res, dt_res, rf_res, lr_res, svm_res]
    class_algos = ['KNN', 'NB', 'DT', 'RF', 'LR', 'SVM']
    
    
    for res, algo in zip(all_class_res, class_algos):
        # print(algo)
        res_formatted = {
            'Classifier': algo,
            'Acc_test': res['proportion_correct_test_label'],
            'Fold': fold_id,
        }
        
        # some algorithm doesn't have tuning
        if 'best_param' in res:    
            res_formatted['Best_param'] = json.dumps(res['best_param'])                
        else:
            res_formatted['Best_param'] = None
        res_formatted['Acc_train'] = res['best_param_train_score']
        
        all_res = all_res.append(res_formatted, ignore_index=True)
        
        # confusion matrix
        conf_mat_res = {
            'Predicted': res['predicted_test_label'],
            'Reference': test_dat_grps,
            'Fold': [fold_id] * len(test_dat_grps),
            'Classifier': [algo] * len(test_dat_grps)
            }
        conf_mat_res = pd.DataFrame(conf_mat_res)
        conf_mat = conf_mat.append(conf_mat_res, ignore_index=True)
        
        res_formatted = pd.DataFrame(res_formatted, index=[0])
        res_formatted.to_csv(save_dir + "/acc_res_" + algo + "_" + fold_id + ".csv")
        conf_mat_res.to_csv(save_dir + "/acc_conf_mat_" + algo + "_" + fold_id + ".csv")

conf_mat.to_csv(save_dir + "/acc_conf_mat.csv")
all_res.to_csv(save_dir + "/acc_res.csv")

# compute average accuracy
all_res['Acc_train'] = [0 if v is None else v for v in all_res[['Acc_train']].to_numpy().ravel()]
avg_per_class = all_res.groupby('Classifier').agg(['mean', 'std'])

# remove sd for test as it makes no sense
avg_per_class.columns = avg_per_class.columns.droplevel()
avg_per_class.columns = ['Acc_train_mean', 'Acc_train_sd', 'Acc_test_mean', 'Acc_test_sd']
avg_per_class = avg_per_class.drop(columns=['Acc_test_sd'])

avg_per_class.to_csv(save_dir + "/acc_avg_per_class.csv")

# compute stat for confusion matrix
# conf_mat.rename(columns={'True':'Reference'}, inplace=True)
conf_mat_stats = conf_mat.groupby(['Classifier', 'Predicted', 'Reference']).count()
conf_mat_stats.rename(columns={'Fold':'count'}, inplace=True)
conf_mat_stats = conf_mat_stats.pivot_table('count', ['Classifier', 'Predicted'], 'Reference')
conf_mat_stats.to_csv(save_dir + "/acc_conf_mat_stats.csv")




